<p align="center" style="border-radius: 10px">
  <img src="asset/logov2.png" width="35%" alt="logo"/>
</p>

# ğŸ“½ï¸ CineTechBench: A Benchmark for Cinematographic Technique Understanding and Generation

<div align="center">
<a href="https://pris-cv.github.io/CineTechBench/"><img src="https://img.shields.io/static/v1?label=Project&message=Github&color=blue&logo=github-pages"></a> &ensp;
<a href=""><img src="https://img.shields.io/static/v1?label=Arxiv&message=CineTechBench&color=red&logo=arxiv"></a> &ensp;
<a href="https://huggingface.co/datasets/Xinran0906/CineTechBench"><img src="https://img.shields.io/static/v1?label=Huggingface Dataset&message=CineTechBench&color=yellow"></a> &ensp;
</div>



## ğŸ‘€ Introduction
We present CineTechBench, a pioneering benchmark founded on precise, manual annotation by seasoned cinematography experts across key cinematography dimensions. Our benchmark covers seven essential aspectsâ€”shot scale, shot angle, composition, camera movement, lighting, color, and focal lengthâ€”and includes over 600 annotated movie images and 120 movie clips with clear cinematographic techniques.




<div style="display: flex; align-items: center; justify-content: center; gap: 20px;">
<!--   <img src="asset/tax.png" width="30%" alt="tax"/> -->
  <img src="asset/bench_compare.png" width="100%" alt="bench_compare"/>
</div>




## ğŸ“Œ TODO
- [ ] Release paper
- [ ] Video extraction script for movie clips
- [ ] Movie image link organization and documentation
- [ ] Question-answering evaluation script
- [ ] Description evaluation script
- [ ] Camera trajectory similarity calculation script




## ğŸ’½ Copyright
We fully respect the copyright of all films and do not use any clips for commercial purposes. Instead of distributing or hosting video content, we only provide links to publicly available, authorized sources (e.g., official studio or distributor channels). All assets are credited to their original rights holders, and our use of these links falls under fairâ€use provisions for nonâ€commercial, academic research.



## ğŸ¤— Acknowledgements
We would like to thank the contributors to the [Wan2.1](https://github.com/Wan-Video/Wan2.1), [FramePack](https://github.com/lllyasviel/FramePack), [CamI2V](https://github.com/ZGCTroy/CamI2V), [vLLM](https://github.com/vllm-project/vllm), [SGLang](https://github.com/sgl-project/sglang), [LMDeploy](https://github.com/InternLM/lmdeploy),  for their open research.

## ğŸ“® Contant

If you have any question please free to mail to wangxr@bupt.edu.cn









