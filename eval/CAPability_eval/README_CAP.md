# CAPability Caption Evaluation

This repository provides an **end-to-end pipeline** for evaluating image-captioning models on **CAPability** .

---

## ðŸ“‚ Directory Layout

```text
CAPability_eval/
â”œâ”€â”€ anno/                         # GT files (created in Step 1)
â”œâ”€â”€ anno_video/                   # GT for video captions (optional)
â”œâ”€â”€ evaluation/                   # CAPability metric implementation
â”‚   â”œâ”€â”€ eval2.py                  # â¬…ï¸Ž main evaluation entry
â”‚   â”œâ”€â”€ OPTIONS.py                # dimension-set for images
â”‚   â”œâ”€â”€ OPTIONS_video.py          # dimension-set for videos
â”‚   â””â”€â”€ prompt2.py                # LLM prompt template
â”‚
â”œâ”€â”€ capability_conda.yml          # Reproducible conda environment
â”œâ”€â”€ eval_run_all.sh               # â¬…ï¸Ž Step 3: call evaluation/eval2.py per model
â”œâ”€â”€ process_all.sh                # Step 2: Convert raw outputs â†’ eval input
â”œâ”€â”€ split_annotation_by_metrics.py# Step 1: Generate GT by metric
â”œâ”€â”€ split_inference_by_model.py    # helper for process_all.sh
â”œâ”€â”€ image_description_annotation.json
â”œâ”€â”€ requirements.txt              # Extra Python dependencies
â””â”€â”€ README_CAP.md                 # (this file)

```

---

## ðŸš€ Quick Start

### 0  Environment Setup

```bash
# âžŠ Create and activate the conda environment
conda env create -f capability_conda.yml
conda activate capability

# âž‹ Install any additional Python packages
pip install -r requirements.txt
```

> **Note**  All subsequent commands assume your working directory is
> `CAPability_eval/`.

### 1  Generate CAPability-Style Annotations
**Place** the `image_description_annotation.json` inside `CAPability_eval/` .
```bash
python split_annotation_by_metrics.py
```

This script slices `image_description_annotation.json` into six JSONL files
(Scale, Angle, Composition, Colors, Lighting, Focal Lengths) under `anno/`.

### 2  Convert Model Outputs â†’ Evaluation Input

1. **Place** the `models_generated_image_description/` folder (containing each modelâ€™s raw image descriptions) inside `inference/` (create `inference/` alongside this README if it does not yet exist).
2. Run the helper script:

```bash
bash ./process_all.sh
```

The script processes **only** the model names hard-coded inside the file.
If you add new model directories to `inference/`, remember to update that list before rerunning.
Standardised inputs are written to `inputs/` (created automatically).

---

## 3  Evaluate All Models

```bash
bash ./eval_run_all.sh
```

The script will

1. **Enumerate** every processed model directory under `inference/processed/`.
2. **Invoke** `python evaluation/eval2.py` for each model with the following args

   ```text
   --caption_file_root   inference/processed/<model_name>
   --gt_file_root        anno/                 # image GT
   --gt_file_root_video  anno_video/           # video GT (if any)
   --save_root           evaluation/<model_name>
   ```
3. **Write** result JSON files per model to `evaluation/<model_name>/`

