# CineTechBench â€“ COCOâ€‘Style Caption Evaluation

This repository provides an **endâ€‘toâ€‘end pipeline** for evaluating imageâ€‘captioning models on **CineTechBench** using the official MSâ€‘COCO metrics.

---

## ðŸ“‚ Directory Layout

```text
CineTechBench/
â””â”€â”€ eval/
    â””â”€â”€ coco_caption_eval/
        â”œâ”€â”€ create/               # Annotationâ€‘creation utilities
        â”‚   â”œâ”€â”€ anno_process.py
        â”‚   â””â”€â”€ res_proceses.py   # (helper for resultâ€‘file postâ€‘processing)
        â”œâ”€â”€ cocoEval.py           # Thin wrapper on COCOEvalCap
        â”œâ”€â”€ coco_conda.yml        # Reproducible conda environment
        â”œâ”€â”€ eval_run_all.sh       # Evaluate every model in one go
        â”œâ”€â”€ generate_all.sh       # Convert raw outputs â†’ COCO result JSON
        â”œâ”€â”€ README_coco.md        # (this file)
        â””â”€â”€ requirements.txt      # Extra Python dependencies
```

---

## ðŸš€ Quick Start

### 0Â Â Environment Setup

```bash
# âžŠÂ Create and activate the conda environment
conda env create -f coco_conda.yml
conda activate coco

# âž‹Â Install any additional Python packages
pip install -r requirements.txt
```

> **Note**Â Â All subsequent commands assume your working directory is
> `CineTechBench/eval/coco_caption_eval`.

### 1Â Â Generate COCOâ€‘Style Annotations

```bash
python create/anno_process.py
```

This script converts your custom descriptions into the standard COCO annotation schema.

### 2Â Â Convert Model Outputs â†’ Result JSON

1. **Place** the `models_generated_image_description/` folder (which contains each modelâ€™s raw image descriptions) inside `res/` (create `res/` alongside this README if it does not yet exist).
2. Run the helper script:

```bash
bash ./generate_all.sh
```

The script processes **only** the model names that are hardâ€‘coded inside the script. If you add new model directories to `res/`, remember to update that list before rerunning. Standardised outputs are written to `results/` (automatically created).

### 3Â Â Evaluate All Models

```bash
bash ./eval_run_all.sh
```

For every result JSON, you will see the standard COCO metrics (BLEUâ€‘1â€‘4, METEOR, ROUGEâ€‘L, CIDEr) inÂ `coco_res/`.

---
